{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-wcl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfkIVUFDgXWp",
        "outputId": "860b3c6e-c2f3-47bf-e2b9-f94bd0acd52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "config = {\n",
        "'MAX_LEN': 128,\n",
        "\"TRAIN_BATCH_SIZE\": 32,\n",
        "\"VALID_BATCH_SIZE\": 8,\n",
        "\"EPOCHS\": 2,\n",
        "\"BASE_MODEL_PATH\": \"bert-base-uncased\",\n",
        "\"MODEL_PATH\": \"model.bin\",\n",
        "\"TRAINING_FILE\": \"../input/ner_dataset.csv\",\n",
        "\"TOKENIZER\": transformers.BertTokenizer.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    do_lower_case=True\n",
        ")}\n",
        "\n",
        "config = dotdict(config)"
      ],
      "metadata": {
        "id": "8djtvRWkgaC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class EntityDataset:\n",
        "    def __init__(self, texts, pos, tags):\n",
        "        # texts: [[\"hi\", \",\", \"my\", \"name\", \"is\", \"abhishek\"], [\"hello\".....]]\n",
        "        # pos/tags: [[1 2 3 4 1 5], [....].....]]\n",
        "        self.texts = texts\n",
        "        self.pos = pos\n",
        "        self.tags = tags\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        pos = self.pos[item]\n",
        "        tags = self.tags[item]\n",
        "\n",
        "        ids = []\n",
        "        target_pos = []\n",
        "        target_tag =[]\n",
        "\n",
        "        for i, s in enumerate(text):\n",
        "            inputs = config.TOKENIZER.encode(\n",
        "                s,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "            input_len = len(inputs)\n",
        "            ids.extend(inputs)\n",
        "            target_pos.extend([pos[i]] * input_len)\n",
        "            target_tag.extend([tags[i]] * input_len)\n",
        "\n",
        "        ids = ids[:config.MAX_LEN - 2]\n",
        "        target_pos = target_pos[:config.MAX_LEN - 2]\n",
        "        target_tag = target_tag[:config.MAX_LEN - 2]\n",
        "\n",
        "        ids = [101] + ids + [102]\n",
        "        target_pos = [0] + target_pos + [0]\n",
        "        target_tag = [0] + target_tag + [0]\n",
        "\n",
        "        mask = [1] * len(ids)\n",
        "        token_type_ids = [0] * len(ids)\n",
        "\n",
        "        padding_len = config.MAX_LEN - len(ids)\n",
        "\n",
        "        ids = ids + ([0] * padding_len)\n",
        "        mask = mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_pos = target_pos + ([0] * padding_len)\n",
        "        target_tag = target_tag + ([0] * padding_len)\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"target_pos\": torch.tensor(target_pos, dtype=torch.long),\n",
        "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
        "        }"
      ],
      "metadata": {
        "id": "NnybEd1ggdgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        _, _, loss = model(**data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        _, _, loss = model(**data)\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "4DlxAcvTgjsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "\n",
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, num_labels)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "class EntityModel(nn.Module):\n",
        "    def __init__(self, num_tag, num_pos):\n",
        "        super(EntityModel, self).__init__()\n",
        "        self.num_tag = num_tag\n",
        "        self.num_pos = num_pos\n",
        "        self.bert = transformers.BertModel.from_pretrained(config.BASE_MODEL_PATH,return_dict=False)\n",
        "        \n",
        "        self.bert_drop_1 = nn.Dropout(0.3)\n",
        "        self.bert_drop_2 = nn.Dropout(0.2)\n",
        "        self.out_tag = nn.Linear(768, self.num_tag)\n",
        "        self.out_pos = nn.Linear(768, self.num_pos)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids, target_pos, target_tag):\n",
        "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        bo_tag = self.bert_drop_1(o1)\n",
        "        bo_pos = self.bert_drop_2(o1)\n",
        "\n",
        "        tag = self.out_tag(bo_tag)\n",
        "        pos = self.out_pos(bo_pos)\n",
        "\n",
        "        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n",
        "        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n",
        "\n",
        "        loss = (loss_tag + loss_pos) / 2\n",
        "        # loss = loss_tag\n",
        "\n",
        "        return tag, pos, loss"
      ],
      "metadata": {
        "id": "Se7QHWWogmCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import joblib\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict(sentence):\n",
        "\n",
        "    meta_data = joblib.load(\"meta.bin\")\n",
        "    enc_pos = meta_data[\"enc_pos\"]\n",
        "    enc_tag = meta_data[\"enc_tag\"]\n",
        "\n",
        "    num_pos = len(list(enc_pos.classes_))\n",
        "    num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "    \n",
        "    tokenized_sentence = config.TOKENIZER.encode(sentence)\n",
        "\n",
        "    sentence = sentence.split()\n",
        "    print(sentence)\n",
        "    print(tokenized_sentence)\n",
        "\n",
        "    test_dataset = EntityDataset(\n",
        "        texts=[sentence], \n",
        "        pos=[[0] * len(sentence)], \n",
        "        tags=[[0] * len(sentence)]\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "    model.load_state_dict(torch.load(config.MODEL_PATH))\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        data = test_dataset[0]\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device).unsqueeze(0)\n",
        "        tag, pos, _ = model(**data)\n",
        "\n",
        "        print(config.TOKENIZER.decode(tokenized_sentence))\n",
        "\n",
        "        print(\n",
        "            enc_tag.inverse_transform(\n",
        "                tag.argmax(2).cpu().numpy().reshape(-1)\n",
        "            )[:len(tokenized_sentence)]\n",
        "        )\n",
        "        print(\n",
        "            enc_pos.inverse_transform(\n",
        "                pos.argmax(2).cpu().numpy().reshape(-1)\n",
        "            )[:len(tokenized_sentence)]\n",
        "        )"
      ],
      "metadata": {
        "id": "JQA1gJ13gp5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import joblib\n",
        "import torch\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def read_wcl_data():\n",
        "        lines = []\n",
        "        data_dir = \"wiki_good.txt\"\n",
        "\n",
        "        with open(data_dir, 'r') as f:\n",
        "            lines = f.read().split(\"# \")\n",
        "\n",
        "        data = []\n",
        "        \n",
        "        for line in lines[1:]:\n",
        "            data.append(line)\n",
        "\n",
        "        return data\n",
        "\n",
        "def build_wcl_pos_features(data):    \n",
        "    def _build_wcl_feature(sent, numbered = True):\n",
        "        sequence = []\n",
        "        hyponym, hypernym, total_tags = \"\", [], []\n",
        "        isHypernymSeq = False\n",
        "        index = 0\n",
        "        label = []\n",
        "\n",
        "        for token in sent.split():\n",
        "            isValid = False\n",
        "\n",
        "            if token == \"<GENUS>\":\n",
        "                isHypernymSeq = True\n",
        "\n",
        "            elif token == \"</HYPER>\":\n",
        "                isHypernymSeq = False\n",
        "            else:\n",
        "                *tags, word = token.split(\"_\")\n",
        "\n",
        "                if len(tags) != 0:\n",
        "                    for c in word:\n",
        "                        if c.isalpha():\n",
        "                            isValid = True\n",
        "                            break\n",
        "                    \n",
        "\n",
        "                    if isValid:\n",
        "                        try:            \n",
        "                            # tag = tags[1] if len(tags) > 1 else tags[0]\n",
        "                            tag = \"_\".join(tags)\n",
        "                        except:\n",
        "                            print(tags)\n",
        "                        if numbered:\n",
        "                            tags += f\"_{index}\"\n",
        "\n",
        "                        if isHypernymSeq:\n",
        "                                label.append('HYPERNYM')\n",
        "                        elif \"TARGET\" in token:\n",
        "                            hyponym = tags\n",
        "                            label.append('HYPONYM')\n",
        "                        else:\n",
        "                            label.append('NO')\n",
        "\n",
        "                        total_tags.append(tag)    \n",
        "                        sequence.append(word)\n",
        "\n",
        "                        index += 1\n",
        "\n",
        "        return sequence, total_tags, label\n",
        "        \n",
        "    return [_build_wcl_feature(item) for item in data]\n",
        "\n",
        "\n",
        "def build_df(transformed):\n",
        "    data = []\n",
        "\n",
        "    for idx, item in enumerate(transformed):\n",
        "      for i in range(0, len(item[0])):\n",
        "        data.append([f\"Sentence {idx + 1}\", item[0][i], item[1][i], item[2][i]])\n",
        "\n",
        "    data = pd.DataFrame(data, columns = ['Sentence #', 'Word', 'POS', \"Tag\"])\n",
        "    return data\n",
        "\n",
        "def process_data(data_path):\n",
        "    data = read_wcl_data()\n",
        "\n",
        "    transformed = build_wcl_pos_features(data)\n",
        "    df = build_df(transformed)\n",
        "\n",
        "    enc_pos = preprocessing.LabelEncoder()\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "\n",
        "    df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"])\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, pos, tag, enc_pos, enc_tag\n",
        "\n",
        "\n",
        "def train():\n",
        "\n",
        "    sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n",
        "    \n",
        "    meta_data = {\n",
        "        \"enc_pos\": enc_pos,\n",
        "        \"enc_tag\": enc_tag\n",
        "    }\n",
        "\n",
        "    joblib.dump(meta_data, \"meta.bin\")\n",
        "\n",
        "    num_pos = len(list(enc_pos.classes_))\n",
        "    num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "    (\n",
        "        train_sentences,\n",
        "        test_sentences,\n",
        "        train_pos,\n",
        "        test_pos,\n",
        "        train_tag,\n",
        "        test_tag\n",
        "    ) = model_selection.train_test_split(sentences, pos, tag, random_state=42, test_size=0.1)\n",
        "\n",
        "    train_dataset = EntityDataset(\n",
        "        texts=train_sentences, pos=train_pos, tags=train_tag\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n",
        "    )\n",
        "\n",
        "    valid_dataset = EntityDataset(\n",
        "        texts=test_sentences, pos=test_pos, tags=test_tag\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "    model.to(device)\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": 0.001,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    num_train_steps = int(len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    best_loss = np.inf\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
        "        test_loss = eval_fn(valid_data_loader, model, device)\n",
        "        print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
        "        if test_loss < best_loss:\n",
        "            torch.save(model.state_dict(), config.MODEL_PATH)\n",
        "            best_loss = test_loss"
      ],
      "metadata": {
        "id": "REWekaQMgtv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PEydg6Gonyz",
        "outputId": "8eba4f0b-79ae-4461-c160-787f6c0f3aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "100%|██████████| 53/53 [01:04<00:00,  1.22s/it]\n",
            "100%|██████████| 24/24 [00:02<00:00,  8.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss = 1.777827870171025 Valid Loss = 1.0983697548508644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 53/53 [01:04<00:00,  1.22s/it]\n",
            "100%|██████████| 24/24 [00:02<00:00,  8.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss = 1.0126786153271514 Valid Loss = 0.8557106703519821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict('canyon - a deep, elongated cavity cut by running water in the roof or floor of a cave or forming a cave passage.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfOs2nMYoy77",
        "outputId": "e9fe8b6b-0790-4fb3-8b1a-a592e2b4e73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['canyon', '-', 'a', 'deep,', 'elongated', 'cavity', 'cut', 'by', 'running', 'water', 'in', 'the', 'roof', 'or', 'floor', 'of', 'a', 'cave', 'or', 'forming', 'a', 'cave', 'passage.']\n",
            "[101, 8399, 1011, 1037, 2784, 1010, 17876, 17790, 3013, 2011, 2770, 2300, 1999, 1996, 4412, 2030, 2723, 1997, 1037, 5430, 2030, 5716, 1037, 5430, 6019, 1012, 102]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] canyon - a deep, elongated cavity cut by running water in the roof or floor of a cave or forming a cave passage. [SEP]\n",
            "['HYPERNYM' 'NO' 'NO' 'HYPERNYM' 'HYPERNYM' 'HYPERNYM' 'HYPERNYM'\n",
            " 'HYPERNYM' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO'\n",
            " 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'HYPERNYM']\n",
            "['' 'NP_NN' 'VP_VBZ' 'NP_DT' 'NP_JJ' 'NP_DT' 'NP_JJ' 'NP_NN' 'VP_VVN'\n",
            " 'PP_IN' 'PP_JJ' 'PP_NN' 'PP_IN' 'PP_DT' 'PP_NN' 'CC' 'PP_NN' 'PP_IN'\n",
            " 'PP_DT' 'PP_NN' 'CC' 'PP_IN' 'PP_DT' 'PP_NN' 'PP_NN' 'PP_IN' '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C"
      ],
      "metadata": {
        "id": "iS07AmFkqxjy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}