{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==2.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYYogFnt7eis",
        "outputId": "3ebb68e6-e3a7-49bc-908c-62763853f3ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==2.6.0\n",
            "  Downloading transformers-2.6.0-py3-none-any.whl (540 kB)\n",
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 102 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 296 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 317 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 409 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 481 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 501 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 522 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 540 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (4.64.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "  Downloading tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 20.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (3.6.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (1.21.6)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.22.3-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 74.1 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.26.0,>=1.25.3\n",
            "  Downloading botocore-1.25.3-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.3->boto3->transformers==2.6.0) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 76.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.3->boto3->transformers==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6.0) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 77.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6.0) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.6.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, tokenizers, sentencepiece, sacremoses, boto3, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.22.3 botocore-1.25.3 jmespath-1.0.0 s3transfer-0.5.2 sacremoses-0.0.49 sentencepiece-0.1.96 tokenizers-0.5.2 transformers-2.6.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "Iv2p__ygsYDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sGjLeNBlHOSx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JgUQDe4q7X-1"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "Y4rnAeDa7bS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6cb29a7-0379-4b07-ca40-1e2c478bf201"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# UTILS\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "class Alphabet:\n",
        "    def __init__(self, name, padflag=True, unkflag=True, keep_growing=True):\n",
        "        self.name = name\n",
        "        self.PAD = \"</pad>\"\n",
        "        self.UNKNOWN = \"</unk>\"\n",
        "        self.padflag = padflag\n",
        "        self.unkflag = unkflag\n",
        "        self.instance2index = {}\n",
        "        self.instances = []\n",
        "        self.keep_growing = keep_growing\n",
        "        self.next_index = 0\n",
        "        self.index_num = {}\n",
        "        if self.padflag:\n",
        "            self.add(self.PAD)\n",
        "        if self.unkflag:\n",
        "            self.add(self.UNKNOWN)\n",
        "\n",
        "    def clear(self, keep_growing=True):\n",
        "        self.instance2index = {}\n",
        "        self.instances = []\n",
        "        self.keep_growing = keep_growing\n",
        "        self.next_index = 0\n",
        "\n",
        "    def add(self, instance):\n",
        "        if instance not in self.instance2index:\n",
        "            self.instances.append(instance)\n",
        "            self.instance2index[instance] = self.next_index\n",
        "            self.index_num[self.next_index] = 1\n",
        "            self.next_index += 1\n",
        "\n",
        "\n",
        "    def get_index(self, instance):\n",
        "        try:\n",
        "            index = self.instance2index[instance]\n",
        "            self.index_num[index] = self.index_num[index] + 1\n",
        "            return index\n",
        "        except KeyError:\n",
        "            if self.keep_growing:\n",
        "                index = self.next_index\n",
        "                self.add(instance)\n",
        "                return index\n",
        "            else:\n",
        "                if self.UNKNOWN in self.instance2index:\n",
        "                    return self.instance2index[self.UNKNOWN]\n",
        "                else:\n",
        "                    print(self.name + \" get_index raise wrong, return 0. Please check it\")\n",
        "                    return 0\n",
        "\n",
        "    def get_instance(self, index):\n",
        "        if index == 0:\n",
        "            if self.padflag:\n",
        "                print(self.name +\" get_instance of </pad>, wrong?\")\n",
        "            if not self.padflag and self.unkflag:\n",
        "                print(self.name +\" get_instance of </unk>, wrong?\")\n",
        "            return self.instances[index]\n",
        "        try:\n",
        "            return self.instances[index]\n",
        "        except IndexError:\n",
        "            # print('WARNING: '+ self.name + ' Alphabet get_instance, unknown instance, return the </unk> label.')\n",
        "            return\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.instances)\n",
        "\n",
        "    def iteritems(self):\n",
        "        return self.instance2index.items()\n",
        "\n",
        "    def enumerate_items(self, start=1):\n",
        "        if start < 1 or start >= self.size():\n",
        "            raise IndexError(\"Enumerate is allowed between [1 : size of the alphabet)\")\n",
        "        return zip(range(start, len(self.instances) + 1), self.instances[start - 1:])\n",
        "\n",
        "    def close(self):\n",
        "        self.keep_growing = False\n",
        "\n",
        "    def open(self):\n",
        "        self.keep_growing = True\n",
        "\n",
        "    def get_content(self):\n",
        "        return {'instance2index': self.instance2index, 'instances': self.instances}\n",
        "\n",
        "    def from_json(self, data):\n",
        "        self.instances = data[\"instances\"]\n",
        "        self.instance2index = data[\"instance2index\"]\n",
        "\n",
        "    def save(self, output_directory, name=None):\n",
        "        \"\"\"\n",
        "        Save both alhpabet records to the given directory.\n",
        "        :param output_directory: Directory to save model and weights.\n",
        "        :param name: The alphabet saving name, optional.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        saving_name = name if name else self.__name\n",
        "        try:\n",
        "            json.dump(self.get_content(), open(os.path.join(output_directory, saving_name + \".json\"), 'w'))\n",
        "        except Exception as e:\n",
        "            print(\"Exception: Alphabet is not saved: \" % repr(e))\n",
        "\n",
        "    def load(self, input_directory, name=None):\n",
        "        \"\"\"\n",
        "        Load model architecture and weights from the give directory. This allow we use old models even the structure\n",
        "        changes.\n",
        "        :param input_directory: Directory to save model and weights\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        loading_name = name if name else self.__name\n",
        "        self.from_json(json.load(open(os.path.join(input_directory, loading_name + \".json\"))))\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value of metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=0):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / (.0001 + self.count)\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        String representation for logging\n",
        "        \"\"\"\n",
        "        # for values that should be recorded exactly e.g. iteration number\n",
        "        if self.count == 0:\n",
        "            return str(self.val)\n",
        "        # for stats\n",
        "        return '%.4f (%.4f)' % (self.val, self.avg)\n",
        "\n",
        "\n",
        "import os, pickle, copy, sys, copy\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "\n",
        "class Data:\n",
        "    def __init__(self):\n",
        "        self.relational_alphabet = Alphabet(\"Relation\", unkflag=False, padflag=False)\n",
        "        self.train_loader = []\n",
        "        self.valid_loader = []\n",
        "        self.test_loader = []\n",
        "        self.weight = {}\n",
        "\n",
        "\n",
        "    def show_data_summary(self):\n",
        "        print(\"DATA SUMMARY START:\")\n",
        "        print(\"     Relation Alphabet Size: %s\" % self.relational_alphabet.size())\n",
        "        print(\"     Train  Instance Number: %s\" % (len(self.train_loader)))\n",
        "        print(\"     Valid  Instance Number: %s\" % (len(self.valid_loader)))\n",
        "        print(\"     Test   Instance Number: %s\" % (len(self.test_loader)))\n",
        "        print(\"DATA SUMMARY END.\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    def generate_instance(self, args, data_process):\n",
        "        tokenizer = BertTokenizer.from_pretrained(args.bert_directory, do_lower_case=False)\n",
        "        if \"train_file\" in args:\n",
        "            self.train_loader = data_process(args.train_file, self.relational_alphabet, tokenizer)\n",
        "            self.weight = copy.deepcopy(self.relational_alphabet.index_num)\n",
        "        if \"valid_file\" in args:\n",
        "            self.valid_loader = data_process(args.valid_file, self.relational_alphabet, tokenizer)\n",
        "        if \"test_file\" in args:\n",
        "            self.test_loader = data_process(args.test_file, self.relational_alphabet, tokenizer)\n",
        "        self.relational_alphabet.close()\n",
        "\n",
        "def build_data(args):\n",
        "\n",
        "    file = args.generated_data_directory + args.dataset_name + \"_\" + args.model_name + \"_data.pickle\"\n",
        "    if os.path.exists(file) and not args.refresh:\n",
        "        data = load_data_setting(args)\n",
        "    else:\n",
        "        data = Data()\n",
        "        data.generate_instance(args, data_process)\n",
        "        save_data_setting(data, args)\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_data_setting(data, args):\n",
        "    new_data = copy.deepcopy(data)\n",
        "    data.show_data_summary()\n",
        "    if not os.path.exists(args.generated_data_directory):\n",
        "        os.makedirs(args.generated_data_directory)\n",
        "    saved_path = args.generated_data_directory + args.dataset_name + \"_\" + args.model_name + \"_data.pickle\"\n",
        "    with open(saved_path, 'wb') as fp:\n",
        "        pickle.dump(new_data, fp)\n",
        "    print(\"Data setting is saved to file: \", saved_path)\n",
        "\n",
        "\n",
        "def load_data_setting(args):\n",
        "\n",
        "    saved_path = args.generated_data_directory + args.dataset_name + \"_\" + args.model_name + \"_data.pickle\"\n",
        "    with open(saved_path, 'rb') as fp:\n",
        "        data = pickle.load(fp)\n",
        "    print(\"Data setting is loaded from file: \", saved_path)\n",
        "    data.show_data_summary()\n",
        "    return data\n",
        "\n",
        "\n",
        "import torch, collections\n",
        "\n",
        "\n",
        "def list_index(list1: list, list2: list) -> list:\n",
        "    try:\n",
        "      start = [i for i, x in enumerate(list2) if x == list1[0]]\n",
        "      end = [i for i, x in enumerate(list2) if x == list1[-1]]\n",
        "      if len(start) == 1 and len(end) == 1:\n",
        "          return start[0], end[0]\n",
        "      else:\n",
        "          index = 0, 1\n",
        "          for i in start:\n",
        "              for j in end:\n",
        "                  if i <= j:\n",
        "                      if list2[i:j+1] == list1:\n",
        "                          index = (i, j)\n",
        "                          break\n",
        "          return index[0], index[1]\n",
        "    except:\n",
        "      print(list1, list2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def list_index(list1: list, list2: list) -> list:\n",
        "#     start = [i for i, x in enumerate(list2) if x == list1[0]]\n",
        "#     end = [i for i, x in enumerate(list2) if x == list1[-1]]\n",
        "#     if len(start) == 1 and len(end) == 1:\n",
        "#         return start[0], end[0]\n",
        "#     else:\n",
        "#         for i in start:\n",
        "#             for j in end:\n",
        "#                 if i <= j:\n",
        "#                     if list2[i:j+1] == list1:\n",
        "#                         break\n",
        "#         return i, j\n",
        "#\n",
        "\n",
        "def remove_accents(text: str) -> str:\n",
        "    accents_translation_table = str.maketrans(\n",
        "    \"áéíóúýàèìòùỳâêîôûŷäëïöüÿñÁÉÍÓÚÝÀÈÌÒÙỲÂÊÎÔÛŶÄËÏÖÜŸ\",\n",
        "    \"aeiouyaeiouyaeiouyaeiouynAEIOUYAEIOUYAEIOUYAEIOUY\"\n",
        "    )\n",
        "    return text.translate(accents_translation_table)\n",
        "\n",
        "\n",
        "def data_process(input_doc, relational_alphabet, tokenizer):\n",
        "    samples = []\n",
        "    with open(input_doc) as f:\n",
        "        lines = f.readlines()\n",
        "        lines = [eval(ele) for ele in lines]\n",
        "    for i in range(len(lines)):\n",
        "        token_sent = [tokenizer.cls_token] + tokenizer.tokenize(remove_accents(lines[i][\"sentText\"])) + [tokenizer.sep_token]\n",
        "        triples = lines[i][\"relationMentions\"]\n",
        "        target = {\"relation\": [], \"head_start_index\": [], \"head_end_index\": [], \"tail_start_index\": [], \"tail_end_index\": []}\n",
        "        for triple in triples:\n",
        "            head_entity = remove_accents(triple[\"em1Text\"])\n",
        "            tail_entity = remove_accents(triple[\"em2Text\"])\n",
        "            head_token = tokenizer.tokenize(head_entity)\n",
        "            tail_token = tokenizer.tokenize(tail_entity)\n",
        "            relation_id = relational_alphabet.get_index(triple[\"label\"])\n",
        "            head_start_index, head_end_index = list_index(head_token, token_sent)\n",
        "            assert head_end_index >= head_start_index\n",
        "            tail_start_index, tail_end_index = list_index(tail_token, token_sent)\n",
        "            assert tail_end_index >= tail_start_index\n",
        "            target[\"relation\"].append(relation_id)\n",
        "            target[\"head_start_index\"].append(head_start_index)\n",
        "            target[\"head_end_index\"].append(head_end_index)\n",
        "            target[\"tail_start_index\"].append(tail_start_index)\n",
        "            target[\"tail_end_index\"].append(tail_end_index)\n",
        "        sent_id = tokenizer.convert_tokens_to_ids(token_sent)\n",
        "        samples.append([i, sent_id, target])\n",
        "    return samples\n",
        "\n",
        "\n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "    return best_indexes\n",
        "\n",
        "\n",
        "def generate_span(start_logits, end_logits, info, args):\n",
        "    seq_lens = info[\"seq_len\"] # including [CLS] and [SEP]\n",
        "    sent_idxes = info[\"sent_idx\"]\n",
        "    _Prediction = collections.namedtuple(\n",
        "        \"Prediction\", [\"start_index\", \"end_index\", \"start_prob\", \"end_prob\"]\n",
        "    )\n",
        "    output = {}\n",
        "    start_probs = start_logits.softmax(-1)\n",
        "    end_probs = end_logits.softmax(-1)\n",
        "    start_probs = start_probs.cpu().tolist()\n",
        "    end_probs = end_probs.cpu().tolist()\n",
        "    for (start_prob, end_prob, seq_len, sent_idx) in zip(start_probs, end_probs, seq_lens, sent_idxes):\n",
        "        output[sent_idx] = {}\n",
        "        for triple_id in range(args.num_generated_triples):\n",
        "            predictions = []\n",
        "            start_indexes = _get_best_indexes(start_prob[triple_id], args.n_best_size)\n",
        "            end_indexes = _get_best_indexes(end_prob[triple_id], args.n_best_size)\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # We could hypothetically create invalid predictions, e.g., predict\n",
        "                    # that the start of the span is in the sentence. We throw out all\n",
        "                    # invalid predictions.\n",
        "                    if start_index >= (seq_len-1): # [SEP]\n",
        "                        continue\n",
        "                    if end_index >= (seq_len-1):\n",
        "                        continue\n",
        "                    if end_index < start_index:\n",
        "                        continue\n",
        "                    length = end_index - start_index + 1\n",
        "                    if length > args.max_span_length:\n",
        "                        continue\n",
        "                    predictions.append(\n",
        "                        _Prediction(\n",
        "                            start_index=start_index,\n",
        "                            end_index=end_index,\n",
        "                            start_prob=start_prob[triple_id][start_index],\n",
        "                            end_prob=end_prob[triple_id][end_index],\n",
        "                        )\n",
        "                    )\n",
        "            output[sent_idx][triple_id] = predictions\n",
        "    return output\n",
        "\n",
        "\n",
        "def generate_relation(pred_rel_logits, info, args):\n",
        "    rel_probs, pred_rels = torch.max(pred_rel_logits.softmax(-1), dim=2)\n",
        "    rel_probs = rel_probs.cpu().tolist()\n",
        "    pred_rels = pred_rels.cpu().tolist()\n",
        "    sent_idxes = info[\"sent_idx\"]\n",
        "    output = {}\n",
        "    _Prediction = collections.namedtuple(\n",
        "        \"Prediction\", [\"pred_rel\", \"rel_prob\"]\n",
        "    )\n",
        "    for (rel_prob, pred_rel, sent_idx) in zip(rel_probs, pred_rels, sent_idxes):\n",
        "        output[sent_idx] = {}\n",
        "        for triple_id in range(args.num_generated_triples):\n",
        "            output[sent_idx][triple_id] = _Prediction(\n",
        "                            pred_rel=pred_rel[triple_id],\n",
        "                            rel_prob=rel_prob[triple_id])\n",
        "    return output\n",
        "\n",
        "\n",
        "def generate_triple(output, info, args, num_classes):\n",
        "    _Pred_Triple = collections.namedtuple(\n",
        "        \"Pred_Triple\", [\"pred_rel\", \"rel_prob\", \"head_start_index\", \"head_end_index\", \"head_start_prob\", \"head_end_prob\", \"tail_start_index\", \"tail_end_index\", \"tail_start_prob\", \"tail_end_prob\"]\n",
        "    )\n",
        "    pred_head_ent_dict = generate_span(output[\"head_start_logits\"], output[\"head_end_logits\"], info, args)\n",
        "    pred_tail_ent_dict = generate_span(output[\"tail_start_logits\"], output[\"tail_end_logits\"], info, args)\n",
        "    pred_rel_dict = generate_relation(output['pred_rel_logits'], info, args)\n",
        "    triples = {}\n",
        "    for sent_idx in pred_rel_dict:\n",
        "        triples[sent_idx] = []\n",
        "        for triple_id in range(args.num_generated_triples):\n",
        "            pred_rel = pred_rel_dict[sent_idx][triple_id]\n",
        "            pred_head = pred_head_ent_dict[sent_idx][triple_id]\n",
        "            pred_tail = pred_tail_ent_dict[sent_idx][triple_id]\n",
        "            triple = generate_strategy(pred_rel, pred_head, pred_tail, num_classes, _Pred_Triple)\n",
        "            if triple:\n",
        "                triples[sent_idx].append(triple)\n",
        "    # print(triples)\n",
        "    return triples\n",
        "\n",
        "\n",
        "def generate_strategy(pred_rel, pred_head, pred_tail, num_classes, _Pred_Triple):\n",
        "    if pred_rel.pred_rel != num_classes:\n",
        "        if pred_head and pred_tail:\n",
        "            for ele in pred_head:\n",
        "                if ele.start_index != 0:\n",
        "                    break\n",
        "            head = ele\n",
        "            for ele in pred_tail:\n",
        "                if ele.start_index != 0:\n",
        "                    break\n",
        "            tail = ele\n",
        "            return _Pred_Triple(pred_rel=pred_rel.pred_rel, rel_prob=pred_rel.rel_prob, head_start_index=head.start_index, head_end_index=head.end_index, head_start_prob=head.start_prob, head_end_prob=head.end_prob, tail_start_index=tail.start_index, tail_end_index=tail.end_index, tail_start_prob=tail.start_prob, tail_end_prob=tail.end_prob)\n",
        "        else:\n",
        "            return\n",
        "    else:\n",
        "        return\n",
        "\n",
        "\n",
        "# def strict_strategy(pred_rel, pred_head, pred_tail, num_classes, _Pred_Triple):\n",
        "#     if pred_rel.pred_rel != num_classes:\n",
        "#         if pred_head and pred_tail:\n",
        "#             if pred_head[0].start_index != 0 and pred_tail[0].start_index != 0:\n",
        "#                 return _Pred_Triple(pred_rel=pred_rel.pred_rel, rel_prob=pred_rel.rel_prob, head_start_index=pred_head[0].start_index, head_end_index=pred_head[0].end_index, head_start_prob=pred_head[0].start_prob, head_end_prob=pred_head[0].end_prob, tail_start_index=pred_tail[0].start_index, tail_end_index=pred_tail[0].end_index, tail_start_prob=pred_tail[0].start_prob, tail_end_prob=pred_tail[0].end_prob)\n",
        "#             else:\n",
        "#                 return\n",
        "#         else:\n",
        "#             return\n",
        "#     else:\n",
        "#         return\n",
        "\n",
        "\n",
        "def formulate_gold(target, info):\n",
        "    sent_idxes = info[\"sent_idx\"]\n",
        "    gold = {}\n",
        "    for i in range(len(sent_idxes)):\n",
        "        gold[sent_idxes[i]] = []\n",
        "        for j in range(len(target[i][\"relation\"])):\n",
        "            gold[sent_idxes[i]].append(\n",
        "                (target[i][\"relation\"][j].item(), target[i][\"head_start_index\"][j].item(), target[i][\"head_end_index\"][j].item(), target[i][\"tail_start_index\"][j].item(), target[i][\"tail_end_index\"][j].item())\n",
        "            )\n",
        "    return gold\n",
        "\n",
        "\n",
        "def metric(pred, gold):\n",
        "    assert pred.keys() == gold.keys()\n",
        "    gold_num = 0\n",
        "    rel_num = 0\n",
        "    ent_num = 0\n",
        "    right_num = 0\n",
        "    pred_num = 0\n",
        "    for sent_idx in pred:\n",
        "        gold_num += len(gold[sent_idx])\n",
        "        pred_correct_num = 0\n",
        "        prediction = list(set([(ele.pred_rel, ele.head_start_index, ele.head_end_index, ele.tail_start_index, ele.tail_end_index) for ele in pred[sent_idx]]))\n",
        "        pred_num += len(prediction)\n",
        "        for ele in prediction:\n",
        "            if ele in gold[sent_idx]:\n",
        "                right_num += 1\n",
        "                pred_correct_num += 1\n",
        "            if ele[0] in [e[0] for e in gold[sent_idx]]:\n",
        "                rel_num += 1\n",
        "            if ele[1:] in [e[1:] for e in gold[sent_idx]]:\n",
        "                ent_num += 1\n",
        "        # if pred_correct_num != len(gold[sent_idx]):\n",
        "        #     print(\"Gold: \", gold[sent_idx])\n",
        "        #     print(\"Pred: \", prediction)\n",
        "        #     print(pred[sent_idx])\n",
        "    if pred_num == 0:\n",
        "        precision = -1\n",
        "        r_p = -1\n",
        "        e_p = -1\n",
        "    else:\n",
        "        precision = (right_num + 0.0) / pred_num\n",
        "        e_p = (ent_num + 0.0) / pred_num\n",
        "        r_p = (rel_num + 0.0) / pred_num\n",
        "\n",
        "    if gold_num == 0:\n",
        "        recall = -1\n",
        "        r_r = -1\n",
        "        e_r = -1\n",
        "    else:\n",
        "        recall = (right_num + 0.0) / gold_num\n",
        "        e_r = ent_num / gold_num\n",
        "        r_r = rel_num / gold_num\n",
        "\n",
        "    if (precision == -1) or (recall == -1) or (precision + recall) <= 0.:\n",
        "        f_measure = -1\n",
        "    else:\n",
        "        f_measure = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    if (e_p == -1) or (e_r == -1) or (e_p + e_r) <= 0.:\n",
        "        e_f = -1\n",
        "    else:\n",
        "        e_f = 2 * e_r * e_p / (e_p + e_r)\n",
        "\n",
        "    if (r_p == -1) or (r_r == -1) or (r_p + r_r) <= 0.:\n",
        "        r_f = -1\n",
        "    else:\n",
        "        r_f = 2 * r_p * r_r / (r_r + r_p)\n",
        "    print(\"gold_num = \", gold_num, \" pred_num = \", pred_num, \" right_num = \", right_num, \" relation_right_num = \", rel_num, \" entity_right_num = \", ent_num)\n",
        "    print(\"precision = \", precision, \" recall = \", recall, \" f1_value = \", f_measure)\n",
        "    print(\"rel_precision = \", r_p, \" rel_recall = \", r_r, \" rel_f1_value = \", r_f)\n",
        "    print(\"ent_precision = \", e_p, \" ent_recall = \", e_r, \" ent_f1_value = \", e_f)\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f_measure}\n",
        "\n",
        "\n",
        "def num_metric(pred, gold):\n",
        "    test_1, test_2, test_3, test_4, test_other = [], [], [], [], []\n",
        "    for sent_idx in gold:\n",
        "        if len(gold[sent_idx]) == 1:\n",
        "            test_1.append(sent_idx)\n",
        "        elif len(gold[sent_idx]) == 2:\n",
        "            test_2.append(sent_idx)\n",
        "        elif len(gold[sent_idx]) == 3:\n",
        "            test_3.append(sent_idx)\n",
        "        elif len(gold[sent_idx]) == 4:\n",
        "            test_4.append(sent_idx)\n",
        "        else:\n",
        "            test_other.append(sent_idx)\n",
        "\n",
        "    pred_1 = get_key_val(pred, test_1)\n",
        "    gold_1 = get_key_val(gold, test_1)\n",
        "    pred_2 = get_key_val(pred, test_2)\n",
        "    gold_2 = get_key_val(gold, test_2)\n",
        "    pred_3 = get_key_val(pred, test_3)\n",
        "    gold_3 = get_key_val(gold, test_3)\n",
        "    pred_4 = get_key_val(pred, test_4)\n",
        "    gold_4 = get_key_val(gold, test_4)\n",
        "    pred_other = get_key_val(pred, test_other)\n",
        "    gold_other = get_key_val(gold, test_other)\n",
        "    # pred_other = dict((key, vals) for key, vals in pred.items() if key in test_other)\n",
        "    # gold_other = dict((key, vals) for key, vals in gold.items() if key in test_other)\n",
        "    print(\"--*--*--Num of Gold Triplet is 1--*--*--\")\n",
        "    _ = metric(pred_1, gold_1)\n",
        "    \"\"\"print(\"--*--*--Num of Gold Triplet is 2--*--*--\")\n",
        "    _ = metric(pred_2, gold_2)\n",
        "    print(\"--*--*--Num of Gold Triplet is 3--*--*--\")\n",
        "    _ = metric(pred_3, gold_3)\n",
        "    print(\"--*--*--Num of Gold Triplet is 4--*--*--\")\n",
        "    _ = metric(pred_4, gold_4)\n",
        "    print(\"--*--*--Num of Gold Triplet is greater than or equal to 5--*--*--\")\n",
        "    _ = metric(pred_other, gold_other)\"\"\"\n",
        "\n",
        "\n",
        "def overlap_metric(pred, gold):\n",
        "    normal_idx, multi_label_idx, overlap_idx = [], [], []\n",
        "    for sent_idx in gold:\n",
        "        triplets = gold[sent_idx]\n",
        "        if is_normal_triplet(triplets):\n",
        "            normal_idx.append(sent_idx)\n",
        "        if is_multi_label(triplets):\n",
        "            multi_label_idx.append(sent_idx)\n",
        "        if is_overlapping(triplets):\n",
        "            overlap_idx.append(sent_idx)\n",
        "    pred_normal = get_key_val(pred, normal_idx)\n",
        "    gold_normal = get_key_val(gold, normal_idx)\n",
        "    pred_multilabel = get_key_val(pred, multi_label_idx)\n",
        "    gold_multilabel = get_key_val(gold, multi_label_idx)\n",
        "    pred_overlap = get_key_val(pred, overlap_idx)\n",
        "    gold_overlap = get_key_val(gold, overlap_idx)\n",
        "    print(\"--*--*--Normal Triplets--*--*--\")\n",
        "    _ = metric(pred_normal, gold_normal)\n",
        "    print(\"--*--*--Multiply label Triplets--*--*--\")\n",
        "    _ = metric(pred_multilabel, gold_multilabel)\n",
        "    print(\"--*--*--Overlapping Triplets--*--*--\")\n",
        "    _ = metric(pred_overlap, gold_overlap)\n",
        "\n",
        "\n",
        "\n",
        "def is_normal_triplet(triplets):\n",
        "    entities = set()\n",
        "    for triplet in triplets:\n",
        "        head_entity = (triplet[1], triplet[2])\n",
        "        tail_entity = (triplet[3], triplet[4])\n",
        "        entities.add(head_entity)\n",
        "        entities.add(tail_entity)\n",
        "    return len(entities) == 2 * len(triplets)\n",
        "\n",
        "\n",
        "def is_multi_label(triplets):\n",
        "    if is_normal_triplet(triplets):\n",
        "        return False\n",
        "    entity_pair = [(triplet[1], triplet[2], triplet[3], triplet[4]) for triplet in triplets]\n",
        "    return len(entity_pair) != len(set(entity_pair))\n",
        "\n",
        "\n",
        "def is_overlapping(triplets):\n",
        "    if is_normal_triplet(triplets):\n",
        "        return False\n",
        "    entity_pair = [(triplet[1], triplet[2], triplet[3], triplet[4]) for triplet in triplets]\n",
        "    entity_pair = set(entity_pair)\n",
        "    entities = []\n",
        "    for pair in entity_pair:\n",
        "        entities.append((pair[0], pair[1]))\n",
        "        entities.append((pair[2], pair[3]))\n",
        "    entities = set(entities)\n",
        "    return len(entities) != 2 * len(entity_pair)\n",
        "\n",
        "\n",
        "def get_key_val(dict_1, list_1):\n",
        "    dict_2 = dict()\n",
        "    for ele in list_1:\n",
        "        dict_2.update({ele: dict_1[ele]})\n",
        "    return dict_2\n",
        "\n"
      ],
      "metadata": {
        "id": "vXfnLoUI8oNm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Modules to compute the matching cost and solve the corresponding LSAP.\n",
        "\"\"\"\n",
        "import torch\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
        "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
        "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
        "    while the others are un-matched (and thus treated as non-objects).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, loss_weight, matcher):\n",
        "        super().__init__()\n",
        "        self.cost_relation = loss_weight[\"relation\"]\n",
        "        self.cost_head = loss_weight[\"head_entity\"]\n",
        "        self.cost_tail = loss_weight[\"tail_entity\"]\n",
        "        self.matcher = matcher\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" Performs the matching\n",
        "\n",
        "        Params:\n",
        "            outputs: This is a dict that contains at least these entries:\n",
        "                 \"pred_rel_logits\": Tensor of dim [batch_size, num_generated_triples, num_classes] with the classification logits\n",
        "                 \"{head, tail}_{start, end}_logits\": Tensor of dim [batch_size, num_generated_triples, seq_len] with the predicted index logits\n",
        "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict\n",
        "        Returns:\n",
        "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
        "                - index_i is the indices of the selected predictions (in order)\n",
        "                - index_j is the indices of the corresponding selected targets (in order)\n",
        "            For each batch element, it holds:\n",
        "                len(index_i) = len(index_j) = min(num_generated_triples, num_gold_triples)\n",
        "        \"\"\"\n",
        "        bsz, num_generated_triples = outputs[\"pred_rel_logits\"].shape[:2]\n",
        "        # We flatten to compute the cost matrices in a batch\n",
        "        pred_rel = outputs[\"pred_rel_logits\"].flatten(0, 1).softmax(-1)  # [bsz * num_generated_triples, num_classes]\n",
        "        gold_rel = torch.cat([v[\"relation\"] for v in targets])\n",
        "        # after masking the pad token\n",
        "        pred_head_start = outputs[\"head_start_logits\"].flatten(0, 1).softmax(-1)  # [bsz * num_generated_triples, seq_len]\n",
        "        pred_head_end = outputs[\"head_end_logits\"].flatten(0, 1).softmax(-1)\n",
        "        pred_tail_start = outputs[\"tail_start_logits\"].flatten(0, 1).softmax(-1)\n",
        "        pred_tail_end = outputs[\"tail_end_logits\"].flatten(0, 1).softmax(-1)\n",
        "\n",
        "        gold_head_start = torch.cat([v[\"head_start_index\"] for v in targets])\n",
        "        gold_head_end = torch.cat([v[\"head_end_index\"] for v in targets])\n",
        "        gold_tail_start = torch.cat([v[\"tail_start_index\"] for v in targets])\n",
        "        gold_tail_end = torch.cat([v[\"tail_end_index\"] for v in targets])\n",
        "        if self.matcher == \"avg\":\n",
        "            cost = - self.cost_relation * pred_rel[:, gold_rel] - self.cost_head * 1/2 * (pred_head_start[:, gold_head_start] + pred_head_end[:, gold_head_end]) - self.cost_tail * 1/2 * (pred_tail_start[:, gold_tail_start] + pred_tail_end[:, gold_tail_end])\n",
        "        elif self.matcher == \"min\":\n",
        "            cost = torch.cat([pred_head_start[:, gold_head_start].unsqueeze(1), pred_rel[:, gold_rel].unsqueeze(1), pred_head_end[:, gold_head_end].unsqueeze(1), pred_tail_start[:, gold_tail_start].unsqueeze(1), pred_tail_end[:, gold_tail_end].unsqueeze(1)], dim=1)\n",
        "            cost = - torch.min(cost, dim=1)[0]\n",
        "        else:\n",
        "            raise ValueError(\"Wrong matcher\")\n",
        "        cost = cost.view(bsz, num_generated_triples, -1).cpu()\n",
        "        num_gold_triples = [len(v[\"relation\"]) for v in targets]\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost.split(num_gold_triples, -1))]\n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n"
      ],
      "metadata": {
        "id": "9CD2JHLL7g7b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "\n",
        "class SeqEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(SeqEncoder, self).__init__()\n",
        "        self.args = args\n",
        "        self.bert = BertModel.from_pretrained(args.bert_directory)\n",
        "        if args.fix_bert_embeddings:\n",
        "            self.bert.embeddings.word_embeddings.weight.requires_grad = False\n",
        "            self.bert.embeddings.position_embeddings.weight.requires_grad = False\n",
        "            self.bert.embeddings.token_type_embeddings.weight.requires_grad = False\n",
        "        self.config = self.bert.config\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        last_hidden_state, pooler_output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        return last_hidden_state, pooler_output"
      ],
      "metadata": {
        "id": "HxwPhhij8AEq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch, math\n",
        "\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\" This class computes the loss for Set_RE.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class, subject position and object position)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, loss_weight, na_coef, losses, matcher):\n",
        "        \"\"\" Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of relation categories\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            loss_weight: dict containing as key the names of the losses and as values their relative weight.\n",
        "            na_coef: list containg the relative classification weight applied to the NA category and positional classification weight applied to the [SEP]\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.loss_weight = loss_weight\n",
        "        self.matcher = HungarianMatcher(loss_weight, matcher)\n",
        "        self.losses = losses\n",
        "        rel_weight = torch.ones(self.num_classes + 1)\n",
        "        rel_weight[-1] = na_coef\n",
        "        self.register_buffer('rel_weight', rel_weight)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs, targets)\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            if loss == \"entity\" and self.empty_targets(targets):\n",
        "                pass\n",
        "            else:\n",
        "                losses.update(self.get_loss(loss, outputs, targets, indices))\n",
        "        losses = sum(losses[k] * self.loss_weight[k] for k in losses.keys() if k in self.loss_weight)\n",
        "        return losses\n",
        "\n",
        "    def relation_loss(self, outputs, targets, indices):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"relation\" containing a tensor of dim [bsz]\n",
        "        \"\"\"\n",
        "        src_logits = outputs['pred_rel_logits'] # [bsz, num_generated_triples, num_rel+1]\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"relation\"][i] for t, (_, i) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "        loss = F.cross_entropy(src_logits.flatten(0, 1), target_classes.flatten(0, 1), weight=self.rel_weight)\n",
        "        losses = {'relation': loss}\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices):\n",
        "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty triples\n",
        "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
        "        \"\"\"\n",
        "        pred_rel_logits = outputs['pred_rel_logits']\n",
        "        device = pred_rel_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        card_pred = (pred_rel_logits.argmax(-1) != pred_rel_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices,  **kwargs):\n",
        "        loss_map = {\n",
        "            'relation': self.relation_loss,\n",
        "            'cardinality': self.loss_cardinality,\n",
        "            'entity': self.entity_loss\n",
        "        }\n",
        "        return loss_map[loss](outputs, targets, indices, **kwargs)\n",
        "\n",
        "    def entity_loss(self, outputs, targets, indices):\n",
        "        \"\"\"Compute the losses related to the position of head entity or tail entity\n",
        "        \"\"\"\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        selected_pred_head_start = outputs[\"head_start_logits\"][idx]\n",
        "        selected_pred_head_end = outputs[\"head_end_logits\"][idx]\n",
        "        selected_pred_tail_start = outputs[\"tail_start_logits\"][idx]\n",
        "        selected_pred_tail_end = outputs[\"tail_end_logits\"][idx]\n",
        "\n",
        "        target_head_start = torch.cat([t[\"head_start_index\"][i] for t, (_, i) in zip(targets, indices)])\n",
        "        target_head_end = torch.cat([t[\"head_end_index\"][i] for t, (_, i) in zip(targets, indices)])\n",
        "        target_tail_start = torch.cat([t[\"tail_start_index\"][i] for t, (_, i) in zip(targets, indices)])\n",
        "        target_tail_end = torch.cat([t[\"tail_end_index\"][i] for t, (_, i) in zip(targets, indices)])\n",
        "\n",
        "\n",
        "        head_start_loss = F.cross_entropy(selected_pred_head_start, target_head_start)\n",
        "        head_end_loss = F.cross_entropy(selected_pred_head_end, target_head_end)\n",
        "        tail_start_loss = F.cross_entropy(selected_pred_tail_start, target_tail_start)\n",
        "        tail_end_loss = F.cross_entropy(selected_pred_tail_end, target_tail_end)\n",
        "        losses = {'head_entity': 1/2*(head_start_loss + head_end_loss), \"tail_entity\": 1/2*(tail_start_loss + tail_end_loss)}\n",
        "        # print(losses)\n",
        "        return losses\n",
        "\n",
        "    @staticmethod\n",
        "    def empty_targets(targets):\n",
        "        flag = True\n",
        "        for target in targets:\n",
        "            if len(target[\"relation\"]) != 0:\n",
        "                flag = False\n",
        "                break\n",
        "        return flag\n"
      ],
      "metadata": {
        "id": "IhwbOxz68BVj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from transformers.modeling_bert import BertIntermediate, BertOutput, BertAttention\n",
        "\n",
        "\n",
        "class SetDecoder(nn.Module):\n",
        "    def __init__(self, config, num_generated_triples, num_layers, num_classes, return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.return_intermediate = return_intermediate\n",
        "        self.num_generated_triples = num_generated_triples\n",
        "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(num_layers)])\n",
        "        self.LayerNorm =  torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.query_embed = nn.Embedding(num_generated_triples, config.hidden_size)\n",
        "        self.decoder2class = nn.Linear(config.hidden_size, num_classes + 1)\n",
        "        self.decoder2span = nn.Linear(config.hidden_size, 4)\n",
        "\n",
        "        self.head_start_metric_1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.head_end_metric_1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.tail_start_metric_1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.tail_end_metric_1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.head_start_metric_2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.head_end_metric_2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.tail_start_metric_2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.tail_end_metric_2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.head_start_metric_3 = nn.Linear(config.hidden_size, 1, bias=False)\n",
        "        self.head_end_metric_3 = nn.Linear(config.hidden_size, 1, bias=False)\n",
        "        self.tail_start_metric_3 = nn.Linear(config.hidden_size, 1, bias=False)\n",
        "        self.tail_end_metric_3 = nn.Linear(config.hidden_size, 1, bias=False)\n",
        "        \n",
        "        torch.nn.init.orthogonal_(self.head_start_metric_1.weight, gain=1)\n",
        "        torch.nn.init.orthogonal_(self.head_end_metric_1.weight, gain=1)\n",
        "        torch.nn.init.orthogonal_(self.tail_start_metric_1.weight, gain=1)\n",
        "        torch.nn.init.orthogonal_(self.tail_end_metric_1.weight, gain=1)\n",
        "        torch.nn.init.orthogonal_(self.head_start_metric_2.weight, gain=1)\n",
        "        torch.nn.init.orthogonal_(self.head_end_metric_2.weight, gain=1)\n",
        "        torch.nn.init.orthogonal_(self.tail_start_metric_2.weight, gain=1)\n",
        "        torch.nn.init.orthogonal_(self.tail_end_metric_2.weight, gain=1)\n",
        "        torch.nn.init.orthogonal_(self.query_embed.weight, gain=1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, encoder_hidden_states, encoder_attention_mask):\n",
        "        bsz = encoder_hidden_states.size()[0]\n",
        "        hidden_states = self.query_embed.weight.unsqueeze(0).repeat(bsz, 1, 1)\n",
        "        hidden_states = self.dropout(self.LayerNorm(hidden_states))\n",
        "        all_hidden_states = ()\n",
        "        for i, layer_module in enumerate(self.layers):\n",
        "            if self.return_intermediate:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states, encoder_hidden_states, encoder_attention_mask\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "        class_logits = self.decoder2class(hidden_states)\n",
        "        \n",
        "        head_start_logits = self.head_start_metric_3(torch.tanh(\n",
        "            self.head_start_metric_1(hidden_states).unsqueeze(2) + self.head_start_metric_2(\n",
        "                encoder_hidden_states).unsqueeze(1))).squeeze()\n",
        "        head_end_logits = self.head_end_metric_3(torch.tanh(\n",
        "            self.head_end_metric_1(hidden_states).unsqueeze(2) + self.head_end_metric_2(\n",
        "                encoder_hidden_states).unsqueeze(1))).squeeze()\n",
        "\n",
        "        tail_start_logits = self.tail_start_metric_3(torch.tanh(\n",
        "            self.tail_start_metric_1(hidden_states).unsqueeze(2) + self.tail_start_metric_2(\n",
        "                encoder_hidden_states).unsqueeze(1))).squeeze()\n",
        "        tail_end_logits = self.tail_end_metric_3(torch.tanh(\n",
        "            self.tail_end_metric_1(hidden_states).unsqueeze(2) + self.tail_end_metric_2(\n",
        "                encoder_hidden_states).unsqueeze(1))).squeeze()\n",
        "\n",
        "        return class_logits, head_start_logits, head_end_logits, tail_start_logits, tail_end_logits\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.crossattention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attention_mask\n",
        "    ):\n",
        "        self_attention_outputs = self.attention(hidden_states)\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "        if encoder_attention_mask.dim() == 3:\n",
        "            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
        "        elif encoder_attention_mask.dim() == 2:\n",
        "            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(\n",
        "                    encoder_hidden_shape, encoder_attention_mask.shape\n",
        "                )\n",
        "            )\n",
        "        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -10000.0\n",
        "        cross_attention_outputs = self.crossattention(\n",
        "            hidden_states=attention_output, encoder_hidden_states=encoder_hidden_states,  encoder_attention_mask=encoder_extended_attention_mask\n",
        "        )\n",
        "        attention_output = cross_attention_outputs[0]\n",
        "        outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n",
        "\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        outputs = (layer_output,) + outputs\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Wfs2PL4X8DLq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "\n",
        "class SetPred4RE(nn.Module):\n",
        "\n",
        "    def __init__(self, args, num_classes):\n",
        "        super(SetPred4RE, self).__init__()\n",
        "        self.args = args\n",
        "        self.encoder = SeqEncoder(args)\n",
        "        config = self.encoder.config\n",
        "        self.num_classes = num_classes\n",
        "        self.decoder = SetDecoder(config, args.num_generated_triples, args.num_decoder_layers, num_classes, return_intermediate=False)\n",
        "        self.criterion = SetCriterion(num_classes,  loss_weight=self.get_loss_weight(args), na_coef=args.na_rel_coef, losses=[\"entity\", \"relation\"], matcher=args.matcher)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, targets=None):\n",
        "        last_hidden_state, pooler_output = self.encoder(input_ids, attention_mask)\n",
        "        class_logits, head_start_logits, head_end_logits, tail_start_logits, tail_end_logits = self.decoder(encoder_hidden_states=last_hidden_state, encoder_attention_mask=attention_mask)\n",
        "        # head_start_logits, head_end_logits, tail_start_logits, tail_end_logits = span_logits.split(1, dim=-1)\n",
        "        head_start_logits = head_start_logits.squeeze(-1).masked_fill((1 - attention_mask.unsqueeze(1)).bool(), -10000.0)\n",
        "        head_end_logits = head_end_logits.squeeze(-1).masked_fill((1 - attention_mask.unsqueeze(1)).bool(), -10000.0)\n",
        "        tail_start_logits = tail_start_logits.squeeze(-1).masked_fill((1 - attention_mask.unsqueeze(1)).bool(), -10000.0)\n",
        "        tail_end_logits = tail_end_logits.squeeze(-1).masked_fill((1 - attention_mask.unsqueeze(1)).bool(), -10000.0) # [bsz, num_generated_triples, seq_len]\n",
        "        outputs = {'pred_rel_logits': class_logits, 'head_start_logits': head_start_logits, 'head_end_logits': head_end_logits, 'tail_start_logits': tail_start_logits, 'tail_end_logits': tail_end_logits}\n",
        "        if targets is not None:\n",
        "            loss = self.criterion(outputs, targets)\n",
        "            return loss, outputs\n",
        "        else:\n",
        "            return outputs\n",
        "\n",
        "    def gen_triples(self, input_ids, attention_mask, info):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.forward(input_ids, attention_mask)\n",
        "            # print(outputs)\n",
        "            pred_triple = generate_triple(outputs, info, self.args, self.num_classes)\n",
        "            # print(pred_triple)\n",
        "        return pred_triple\n",
        "\n",
        "    def batchify(self, batch_list):\n",
        "        batch_size = len(batch_list)\n",
        "        sent_idx = [ele[0] for ele in batch_list]\n",
        "        sent_ids = [ele[1] for ele in batch_list]\n",
        "        targets = [ele[2] for ele in batch_list]\n",
        "        sent_lens = list(map(len, sent_ids))\n",
        "        max_sent_len = max(sent_lens)\n",
        "        input_ids = torch.zeros((batch_size, max_sent_len), requires_grad=False).long()\n",
        "        attention_mask = torch.zeros((batch_size, max_sent_len), requires_grad=False, dtype=torch.float32)\n",
        "        for idx, (seq, seqlen) in enumerate(zip(sent_ids, sent_lens)):\n",
        "            input_ids[idx, :seqlen] = torch.LongTensor(seq)\n",
        "            attention_mask[idx, :seqlen] = torch.FloatTensor([1] * seqlen)\n",
        "        if self.args.use_gpu:\n",
        "            input_ids = input_ids.cuda()\n",
        "            attention_mask = attention_mask.cuda()\n",
        "            targets = [{k: torch.tensor(v, dtype=torch.long, requires_grad=False).cuda() for k, v in t.items()} for t in targets]\n",
        "        else:\n",
        "            targets = [{k: torch.tensor(v, dtype=torch.long, requires_grad=False) for k, v in t.items()} for t in targets]\n",
        "        info = {\"seq_len\": sent_lens, \"sent_idx\": sent_idx}\n",
        "        return input_ids, attention_mask, targets, info\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_loss_weight(args):\n",
        "        return {\"relation\": args.rel_loss_weight, \"head_entity\": args.head_ent_loss_weight, \"tail_entity\": args.tail_ent_loss_weight}\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OEz9fhEQ8EdV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################################\n",
        "## TRAINER \n",
        "###########################################################"
      ],
      "metadata": {
        "id": "POk1qZ7P8HoD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, random, gc\n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW\n",
        "\n",
        "\n",
        "\n",
        "class Trainer(nn.Module):\n",
        "    def __init__(self, model, data, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.model = model\n",
        "        self.data = data\n",
        "\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        component = ['encoder', 'decoder']\n",
        "        grouped_params = [\n",
        "            {\n",
        "                'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay) and component[0] in n],\n",
        "                'weight_decay': args.weight_decay,\n",
        "                'lr': args.encoder_lr\n",
        "            },\n",
        "            {\n",
        "                'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay) and component[0] in n],\n",
        "                'weight_decay': 0.0,\n",
        "                'lr': args.encoder_lr\n",
        "            },\n",
        "            {\n",
        "                'params': [p for n, p in self.model.named_parameters() if\n",
        "                           not any(nd in n for nd in no_decay) and component[1] in n],\n",
        "                'weight_decay': args.weight_decay,\n",
        "                'lr': args.decoder_lr\n",
        "            },\n",
        "            {\n",
        "                'params': [p for n, p in self.model.named_parameters() if\n",
        "                           any(nd in n for nd in no_decay) and component[1] in n],\n",
        "                'weight_decay': 0.0,\n",
        "                'lr': args.decoder_lr\n",
        "            }\n",
        "        ]\n",
        "        if args.optimizer == 'Adam':\n",
        "            self.optimizer = optim.Adam(grouped_params)\n",
        "        elif args.optimizer == 'AdamW':\n",
        "            self.optimizer = AdamW(grouped_params)\n",
        "        else:\n",
        "            raise Exception(\"Invalid optimizer.\")\n",
        "        if args.use_gpu:\n",
        "            self.cuda()\n",
        "\n",
        "    def train_model(self):\n",
        "        best_f1 = 0\n",
        "        train_loader = self.data.train_loader\n",
        "        train_num = len(train_loader)\n",
        "        batch_size = self.args.batch_size\n",
        "        total_batch = train_num // batch_size + 1\n",
        "        # result = self.eval_model(self.data.test_loader)\n",
        "        for epoch in range(self.args.max_epoch):\n",
        "            # Train\n",
        "            self.model.train()\n",
        "            self.model.zero_grad()\n",
        "            self.optimizer = self.lr_decay(self.optimizer, epoch, self.args.lr_decay)\n",
        "            print(\"=== Epoch %d train ===\" % epoch, flush=True)\n",
        "            avg_loss = AverageMeter()\n",
        "            random.shuffle(train_loader)\n",
        "            for batch_id in range(total_batch):\n",
        "                start = batch_id * batch_size\n",
        "                end = (batch_id + 1) * batch_size\n",
        "                if end > train_num:\n",
        "                    end = train_num\n",
        "                train_instance = train_loader[start:end]\n",
        "                # print([ele[0] for ele in train_instance])\n",
        "                if not train_instance:\n",
        "                    continue\n",
        "                input_ids, attention_mask, targets, _ = self.model.batchify(train_instance)\n",
        "                loss, _ = self.model(input_ids, attention_mask, targets)\n",
        "                avg_loss.update(loss.item(), 1)\n",
        "                # Optimize\n",
        "                loss.backward()\n",
        "                if self.args.max_grad_norm != 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n",
        "                if (batch_id + 1) % self.args.gradient_accumulation_steps == 0:\n",
        "                    self.optimizer.step()\n",
        "                    self.model.zero_grad()\n",
        "                if batch_id % 100 == 0 and batch_id != 0:\n",
        "                    print(\"     Instance: %d; loss: %.4f\" % (start, avg_loss.avg), flush=True)\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            # Validation\n",
        "            print(\"=== Epoch %d Validation ===\" % epoch)\n",
        "            result = self.eval_model(self.data.valid_loader)\n",
        "            # Test\n",
        "            # print(\"=== Epoch %d Test ===\" % epoch, flush=True)\n",
        "            # result = self.eval_model(self.data.test_loader)\n",
        "            f1 = result['f1']\n",
        "            if f1 > best_f1:\n",
        "                print(\"Achieving Best Result on Validation Set.\", flush=True)\n",
        "                # torch.save({'state_dict': self.model.state_dict()}, self.args.generated_param_directory + \" %s_%s_epoch_%d_f1_%.4f.model\" %(self.model.name, self.args.dataset_name, epoch, result['f1']))\n",
        "                best_f1 = f1\n",
        "                best_result_epoch = epoch\n",
        "            # if f1 <= 0.3 and epoch >= 10:\n",
        "            #     break\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "        print(\"Best result on validation set is %f achieving at epoch %d.\" % (best_f1, best_result_epoch), flush=True)\n",
        "\n",
        "\n",
        "    def eval_model(self, eval_loader):\n",
        "        self.model.eval()\n",
        "        # print(self.model.decoder.query_embed.weight)\n",
        "        prediction, gold = {}, {}\n",
        "        with torch.no_grad():\n",
        "            batch_size = self.args.batch_size\n",
        "            eval_num = len(eval_loader)\n",
        "            total_batch = eval_num // batch_size + 1\n",
        "            for batch_id in range(total_batch):\n",
        "                start = batch_id * batch_size\n",
        "                end = (batch_id + 1) * batch_size\n",
        "                if end > eval_num:\n",
        "                    end = eval_num\n",
        "                eval_instance = eval_loader[start:end]\n",
        "                if not eval_instance:\n",
        "                    continue\n",
        "                input_ids, attention_mask, target, info = self.model.batchify(eval_instance)\n",
        "                gold.update(formulate_gold(target, info))\n",
        "                # print(target)\n",
        "                gen_triples = self.model.gen_triples(input_ids, attention_mask, info)\n",
        "                prediction.update(gen_triples)\n",
        "        num_metric(prediction, gold)\n",
        "        overlap_metric(prediction, gold)\n",
        "        return metric(prediction, gold)\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.model.load_state_dict(state_dict)\n",
        "\n",
        "    @staticmethod\n",
        "    def lr_decay(optimizer, epoch, decay_rate):\n",
        "        # lr = init_lr * ((1 - decay_rate) ** epoch)\n",
        "        if epoch != 0:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = param_group['lr'] * (1 - decay_rate)\n",
        "                # print(param_group['lr'])\n",
        "        return optimizer\n"
      ],
      "metadata": {
        "id": "RhMB_ALd8-Oc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'bert_directory': 'bert_base_uncased',\n",
        "    'num_generated_triples': 3,\n",
        "    'train_file': 'train.json',\n",
        "    'na_rel_coef': 1,\n",
        "    'max_grad_norm': 1,\n",
        "    'max_epoch': 100,\n",
        "    'max_span_length': 10,\n",
        "    'use_gpu': True\n",
        "}"
      ],
      "metadata": {
        "id": "5CBJhH2o9QWc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse, os, torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n",
        "\n",
        "\n",
        "def add_argument_group(name):\n",
        "    arg = parser.add_argument_group(name)\n",
        "    return arg\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    args, unparsed = parser.parse_known_args()\n",
        "    if len(unparsed) > 1:\n",
        "        print(\"Unparsed args: {}\".format(unparsed))\n",
        "    return args, unparsed\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    data_arg = add_argument_group('Data')\n",
        "    \n",
        "    # data_arg.add_argument('--dataset_name', type=str, default=\"NYT-exact\")\n",
        "    # data_arg.add_argument('--train_file', type=str, default=\"./data/NYT/exact_data/train.json\")\n",
        "    # data_arg.add_argument('--valid_file', type=str, default=\"./data/NYT/exact_data/valid.json\")\n",
        "    # data_arg.add_argument('--test_file', type=str, default=\"./data/NYT/exact_data/test.json\")\n",
        "    \n",
        "    # data_arg.add_argument('--dataset_name', type=str, default=\"NYT-partial\")\n",
        "    # data_arg.add_argument('--train_file', type=str, default=\"./data/NYT/casrel_data/new_train.json\")\n",
        "    # data_arg.add_argument('--valid_file', type=str, default=\"./data/NYT/casrel_data/new_valid.json\")\n",
        "    # data_arg.add_argument('--test_file', type=str, default=\"./data/NYT/casrel_data/new_test.json\")\n",
        "    \n",
        "    \n",
        "    data_arg.add_argument('--dataset_name', type=str, default=\"t\")\n",
        "    data_arg.add_argument('--train_file', type=str, default=\"./train.json\")\n",
        "    data_arg.add_argument('--valid_file', type=str, default=\"./valid.json\")\n",
        "    data_arg.add_argument('--test_file', type=str, default=\"./test.json\")\n",
        "\n",
        "    data_arg.add_argument('--generated_data_directory', type=str, default=\"./\")\n",
        "    data_arg.add_argument('--generated_param_directory', type=str, default=\"./\")\n",
        "    data_arg.add_argument('--bert_directory', type=str, default=\"bert-base-uncased\")\n",
        "    data_arg.add_argument(\"--partial\", type=str2bool, default=False)\n",
        "    learn_arg = add_argument_group('Learning')\n",
        "    learn_arg.add_argument('--model_name', type=str, default=\"Set-Prediction-Networks\")\n",
        "    learn_arg.add_argument('--num_generated_triples', type=int, default=2)\n",
        "    learn_arg.add_argument('--num_decoder_layers', type=int, default=3)\n",
        "    learn_arg.add_argument('--matcher', type=str, default=\"avg\", choices=['avg', 'min'])\n",
        "    learn_arg.add_argument('--na_rel_coef', type=float, default=1)\n",
        "    learn_arg.add_argument('--rel_loss_weight', type=float, default=1)\n",
        "    learn_arg.add_argument('--head_ent_loss_weight', type=float, default=2)\n",
        "    learn_arg.add_argument('--tail_ent_loss_weight', type=float, default=2)\n",
        "    learn_arg.add_argument('--fix_bert_embeddings', type=str2bool, default=True)\n",
        "    learn_arg.add_argument('--batch_size', type=int, default=8)\n",
        "    learn_arg.add_argument('--max_epoch', type=int, default=5)\n",
        "    learn_arg.add_argument('--gradient_accumulation_steps', type=int, default=1)\n",
        "    learn_arg.add_argument('--decoder_lr', type=float, default=5e-5)\n",
        "    learn_arg.add_argument('--encoder_lr', type=float, default=2e-5)\n",
        "    learn_arg.add_argument('--lr_decay', type=float, default=0.01)\n",
        "    learn_arg.add_argument('--weight_decay', type=float, default=1e-5)\n",
        "    learn_arg.add_argument('--max_grad_norm', type=float, default=0)\n",
        "    learn_arg.add_argument('--optimizer', type=str, default='AdamW', choices=['Adam', 'AdamW'])\n",
        "    evaluation_arg = add_argument_group('Evaluation')\n",
        "    evaluation_arg.add_argument('--n_best_size', type=int, default=100)\n",
        "    evaluation_arg.add_argument('--max_span_length', type=int, default=12) #NYT webNLG 10\n",
        "    misc_arg = add_argument_group('MISC')\n",
        "    misc_arg.add_argument('--refresh', type=str2bool, default=False)\n",
        "    misc_arg.add_argument('--use_gpu', type=str2bool, default=True)\n",
        "    misc_arg.add_argument('--visible_gpu', type=int, default=1)\n",
        "    misc_arg.add_argument('--random_seed', type=int, default=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    args, unparsed = get_args()\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.visible_gpu)\n",
        "    for arg in vars(args):\n",
        "        print(arg, \":\",  getattr(args, arg))\n",
        "    set_seed(args.random_seed)\n",
        "    data = build_data(args)\n",
        "    model = SetPred4RE(args, data.relational_alphabet.size())\n",
        "    trainer = Trainer(model, data, args)\n",
        "    trainer.train_model()\n",
        "    print(trainer.eval_model(data.test_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxqX2rZG-ODF",
        "outputId": "9ccf18a0-a04e-4de5-8943-f40d6c2bc441"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unparsed args: ['-f', '/root/.local/share/jupyter/runtime/kernel-ccde8708-73df-4315-bc84-3bff6f3e5113.json']\n",
            "dataset_name : t\n",
            "train_file : ./train.json\n",
            "valid_file : ./valid.json\n",
            "test_file : ./test.json\n",
            "generated_data_directory : ./\n",
            "generated_param_directory : ./\n",
            "bert_directory : bert-base-uncased\n",
            "partial : False\n",
            "model_name : Set-Prediction-Networks\n",
            "num_generated_triples : 2\n",
            "num_decoder_layers : 3\n",
            "matcher : avg\n",
            "na_rel_coef : 1\n",
            "rel_loss_weight : 1\n",
            "head_ent_loss_weight : 2\n",
            "tail_ent_loss_weight : 2\n",
            "fix_bert_embeddings : True\n",
            "batch_size : 8\n",
            "max_epoch : 5\n",
            "gradient_accumulation_steps : 1\n",
            "decoder_lr : 5e-05\n",
            "encoder_lr : 2e-05\n",
            "lr_decay : 0.01\n",
            "weight_decay : 1e-05\n",
            "max_grad_norm : 0\n",
            "optimizer : AdamW\n",
            "n_best_size : 100\n",
            "max_span_length : 12\n",
            "refresh : False\n",
            "use_gpu : True\n",
            "visible_gpu : 1\n",
            "random_seed : 1\n",
            "DATA SUMMARY START:\n",
            "     Relation Alphabet Size: 166\n",
            "     Train  Instance Number: 2910\n",
            "     Valid  Instance Number: 277\n",
            "     Test   Instance Number: 449\n",
            "DATA SUMMARY END.\n",
            "Data setting is saved to file:  ./t_Set-Prediction-Networks_data.pickle\n",
            "=== Epoch 0 train ===\n",
            "     Instance: 800; loss: 6.8076\n",
            "     Instance: 1600; loss: 5.1929\n",
            "     Instance: 2400; loss: 4.4636\n",
            "=== Epoch 0 Validation ===\n",
            "--*--*--Num of Gold Triplet is 1--*--*--\n",
            "gold_num =  195  pred_num =  195  right_num =  19  relation_right_num =  181  entity_right_num =  28\n",
            "precision =  0.09743589743589744  recall =  0.09743589743589744  f1_value =  0.09743589743589744\n",
            "rel_precision =  0.9282051282051282  rel_recall =  0.9282051282051282  rel_f1_value =  0.9282051282051282\n",
            "ent_precision =  0.14358974358974358  ent_recall =  0.14358974358974358  ent_f1_value =  0.14358974358974358\n",
            "--*--*--Normal Triplets--*--*--\n",
            "gold_num =  182  pred_num =  179  right_num =  19  relation_right_num =  177  entity_right_num =  19\n",
            "precision =  0.10614525139664804  recall =  0.1043956043956044  f1_value =  0.10526315789473684\n",
            "rel_precision =  0.9888268156424581  rel_recall =  0.9725274725274725  rel_f1_value =  0.9806094182825486\n",
            "ent_precision =  0.10614525139664804  ent_recall =  0.1043956043956044  ent_f1_value =  0.10526315789473684\n",
            "--*--*--Multiply label Triplets--*--*--\n",
            "gold_num =  326  pred_num =  127  right_num =  60  relation_right_num =  68  entity_right_num =  121\n",
            "precision =  0.47244094488188976  recall =  0.18404907975460122  f1_value =  0.2649006622516556\n",
            "rel_precision =  0.5354330708661418  rel_recall =  0.2085889570552147  rel_f1_value =  0.30022075055187636\n",
            "ent_precision =  0.952755905511811  ent_recall =  0.37116564417177916  ent_f1_value =  0.5342163355408389\n",
            "--*--*--Overlapping Triplets--*--*--\n",
            "gold_num =  364  pred_num =  155  right_num =  66  relation_right_num =  78  entity_right_num =  140\n",
            "precision =  0.4258064516129032  recall =  0.1813186813186813  f1_value =  0.2543352601156069\n",
            "rel_precision =  0.5032258064516129  rel_recall =  0.21428571428571427  rel_f1_value =  0.3005780346820809\n",
            "ent_precision =  0.9032258064516129  ent_recall =  0.38461538461538464  ent_f1_value =  0.5394990366088632\n",
            "gold_num =  546  pred_num =  334  right_num =  85  relation_right_num =  255  entity_right_num =  159\n",
            "precision =  0.25449101796407186  recall =  0.15567765567765568  f1_value =  0.19318181818181818\n",
            "rel_precision =  0.7634730538922155  rel_recall =  0.46703296703296704  rel_f1_value =  0.5795454545454545\n",
            "ent_precision =  0.47604790419161674  ent_recall =  0.29120879120879123  ent_f1_value =  0.36136363636363633\n",
            "Achieving Best Result on Validation Set.\n",
            "=== Epoch 1 train ===\n",
            "     Instance: 800; loss: 1.9467\n",
            "     Instance: 1600; loss: 1.9483\n",
            "     Instance: 2400; loss: 1.9415\n",
            "=== Epoch 1 Validation ===\n",
            "--*--*--Num of Gold Triplet is 1--*--*--\n",
            "gold_num =  195  pred_num =  191  right_num =  29  relation_right_num =  179  entity_right_num =  38\n",
            "precision =  0.1518324607329843  recall =  0.14871794871794872  f1_value =  0.15025906735751293\n",
            "rel_precision =  0.93717277486911  rel_recall =  0.9179487179487179  rel_f1_value =  0.9274611398963731\n",
            "ent_precision =  0.19895287958115182  ent_recall =  0.19487179487179487  ent_f1_value =  0.1968911917098446\n",
            "--*--*--Normal Triplets--*--*--\n",
            "gold_num =  182  pred_num =  176  right_num =  28  relation_right_num =  175  entity_right_num =  28\n",
            "precision =  0.1590909090909091  recall =  0.15384615384615385  f1_value =  0.1564245810055866\n",
            "rel_precision =  0.9943181818181818  rel_recall =  0.9615384615384616  rel_f1_value =  0.9776536312849162\n",
            "ent_precision =  0.1590909090909091  ent_recall =  0.15384615384615385  ent_f1_value =  0.1564245810055866\n",
            "--*--*--Multiply label Triplets--*--*--\n",
            "gold_num =  326  pred_num =  114  right_num =  75  relation_right_num =  92  entity_right_num =  114\n",
            "precision =  0.6578947368421053  recall =  0.23006134969325154  f1_value =  0.34090909090909094\n",
            "rel_precision =  0.8070175438596491  rel_recall =  0.2822085889570552  rel_f1_value =  0.4181818181818181\n",
            "ent_precision =  1.0  ent_recall =  0.3496932515337423  ent_f1_value =  0.5181818181818182\n",
            "--*--*--Overlapping Triplets--*--*--\n",
            "gold_num =  364  pred_num =  146  right_num =  88  relation_right_num =  110  entity_right_num =  140\n",
            "precision =  0.6027397260273972  recall =  0.24175824175824176  f1_value =  0.34509803921568627\n",
            "rel_precision =  0.7534246575342466  rel_recall =  0.3021978021978022  rel_f1_value =  0.4313725490196078\n",
            "ent_precision =  0.958904109589041  ent_recall =  0.38461538461538464  ent_f1_value =  0.5490196078431372\n",
            "gold_num =  546  pred_num =  322  right_num =  116  relation_right_num =  285  entity_right_num =  168\n",
            "precision =  0.36024844720496896  recall =  0.21245421245421245  f1_value =  0.2672811059907834\n",
            "rel_precision =  0.8850931677018633  rel_recall =  0.521978021978022  rel_f1_value =  0.6566820276497697\n",
            "ent_precision =  0.5217391304347826  ent_recall =  0.3076923076923077  ent_f1_value =  0.3870967741935484\n",
            "Achieving Best Result on Validation Set.\n",
            "=== Epoch 2 train ===\n",
            "     Instance: 800; loss: 1.3835\n",
            "     Instance: 1600; loss: 1.3069\n",
            "     Instance: 2400; loss: 1.3282\n",
            "=== Epoch 2 Validation ===\n",
            "--*--*--Num of Gold Triplet is 1--*--*--\n",
            "gold_num =  195  pred_num =  193  right_num =  42  relation_right_num =  179  entity_right_num =  55\n",
            "precision =  0.21761658031088082  recall =  0.2153846153846154  f1_value =  0.21649484536082475\n",
            "rel_precision =  0.927461139896373  rel_recall =  0.9179487179487179  rel_f1_value =  0.922680412371134\n",
            "ent_precision =  0.2849740932642487  ent_recall =  0.28205128205128205  ent_f1_value =  0.28350515463917525\n",
            "--*--*--Normal Triplets--*--*--\n",
            "gold_num =  182  pred_num =  174  right_num =  39  relation_right_num =  173  entity_right_num =  39\n",
            "precision =  0.22413793103448276  recall =  0.21428571428571427  f1_value =  0.2191011235955056\n",
            "rel_precision =  0.9942528735632183  rel_recall =  0.9505494505494505  rel_f1_value =  0.9719101123595506\n",
            "ent_precision =  0.22413793103448276  ent_recall =  0.21428571428571427  ent_f1_value =  0.2191011235955056\n",
            "--*--*--Multiply label Triplets--*--*--\n",
            "gold_num =  326  pred_num =  144  right_num =  97  relation_right_num =  106  entity_right_num =  144\n",
            "precision =  0.6736111111111112  recall =  0.29754601226993865  f1_value =  0.4127659574468085\n",
            "rel_precision =  0.7361111111111112  rel_recall =  0.32515337423312884  rel_f1_value =  0.45106382978723414\n",
            "ent_precision =  1.0  ent_recall =  0.44171779141104295  ent_f1_value =  0.6127659574468085\n",
            "--*--*--Overlapping Triplets--*--*--\n",
            "gold_num =  364  pred_num =  180  right_num =  109  relation_right_num =  122  entity_right_num =  176\n",
            "precision =  0.6055555555555555  recall =  0.29945054945054944  f1_value =  0.4007352941176471\n",
            "rel_precision =  0.6777777777777778  rel_recall =  0.33516483516483514  rel_f1_value =  0.4485294117647059\n",
            "ent_precision =  0.9777777777777777  ent_recall =  0.4835164835164835  ent_f1_value =  0.6470588235294118\n",
            "gold_num =  546  pred_num =  354  right_num =  148  relation_right_num =  295  entity_right_num =  215\n",
            "precision =  0.4180790960451977  recall =  0.27106227106227104  f1_value =  0.32888888888888884\n",
            "rel_precision =  0.8333333333333334  rel_recall =  0.5402930402930403  rel_f1_value =  0.6555555555555556\n",
            "ent_precision =  0.6073446327683616  ent_recall =  0.39377289377289376  ent_f1_value =  0.4777777777777778\n",
            "Achieving Best Result on Validation Set.\n",
            "=== Epoch 3 train ===\n",
            "     Instance: 800; loss: 0.9753\n",
            "     Instance: 1600; loss: 0.9691\n",
            "     Instance: 2400; loss: 0.9680\n",
            "=== Epoch 3 Validation ===\n",
            "--*--*--Num of Gold Triplet is 1--*--*--\n",
            "gold_num =  195  pred_num =  194  right_num =  47  relation_right_num =  181  entity_right_num =  57\n",
            "precision =  0.2422680412371134  recall =  0.24102564102564103  f1_value =  0.2416452442159383\n",
            "rel_precision =  0.9329896907216495  rel_recall =  0.9282051282051282  rel_f1_value =  0.9305912596401028\n",
            "ent_precision =  0.29381443298969073  ent_recall =  0.2923076923076923  ent_f1_value =  0.2930591259640103\n",
            "--*--*--Normal Triplets--*--*--\n",
            "gold_num =  182  pred_num =  177  right_num =  45  relation_right_num =  176  entity_right_num =  45\n",
            "precision =  0.2542372881355932  recall =  0.24725274725274726  f1_value =  0.2506963788300836\n",
            "rel_precision =  0.9943502824858758  rel_recall =  0.967032967032967  rel_f1_value =  0.9805013927576602\n",
            "ent_precision =  0.2542372881355932  ent_recall =  0.24725274725274726  ent_f1_value =  0.2506963788300836\n",
            "--*--*--Multiply label Triplets--*--*--\n",
            "gold_num =  326  pred_num =  143  right_num =  109  relation_right_num =  117  entity_right_num =  143\n",
            "precision =  0.7622377622377622  recall =  0.3343558282208589  f1_value =  0.46481876332622607\n",
            "rel_precision =  0.8181818181818182  rel_recall =  0.3588957055214724  rel_f1_value =  0.4989339019189766\n",
            "ent_precision =  1.0  ent_recall =  0.4386503067484663  ent_f1_value =  0.6098081023454158\n",
            "--*--*--Overlapping Triplets--*--*--\n",
            "gold_num =  364  pred_num =  177  right_num =  123  relation_right_num =  134  entity_right_num =  171\n",
            "precision =  0.6949152542372882  recall =  0.33791208791208793  f1_value =  0.4547134935304991\n",
            "rel_precision =  0.7570621468926554  rel_recall =  0.36813186813186816  rel_f1_value =  0.49537892791127547\n",
            "ent_precision =  0.9661016949152542  ent_recall =  0.4697802197802198  ent_f1_value =  0.6321626617375231\n",
            "gold_num =  546  pred_num =  354  right_num =  168  relation_right_num =  310  entity_right_num =  216\n",
            "precision =  0.4745762711864407  recall =  0.3076923076923077  f1_value =  0.37333333333333335\n",
            "rel_precision =  0.8757062146892656  rel_recall =  0.5677655677655677  rel_f1_value =  0.6888888888888889\n",
            "ent_precision =  0.6101694915254238  ent_recall =  0.3956043956043956  ent_f1_value =  0.48\n",
            "Achieving Best Result on Validation Set.\n",
            "=== Epoch 4 train ===\n",
            "     Instance: 800; loss: 0.7173\n",
            "     Instance: 1600; loss: 0.7906\n",
            "     Instance: 2400; loss: 0.8035\n",
            "=== Epoch 4 Validation ===\n",
            "--*--*--Num of Gold Triplet is 1--*--*--\n",
            "gold_num =  195  pred_num =  195  right_num =  49  relation_right_num =  182  entity_right_num =  59\n",
            "precision =  0.2512820512820513  recall =  0.2512820512820513  f1_value =  0.2512820512820513\n",
            "rel_precision =  0.9333333333333333  rel_recall =  0.9333333333333333  rel_f1_value =  0.9333333333333333\n",
            "ent_precision =  0.30256410256410254  ent_recall =  0.30256410256410254  ent_f1_value =  0.30256410256410254\n",
            "--*--*--Normal Triplets--*--*--\n",
            "gold_num =  182  pred_num =  176  right_num =  45  relation_right_num =  175  entity_right_num =  45\n",
            "precision =  0.2556818181818182  recall =  0.24725274725274726  f1_value =  0.25139664804469275\n",
            "rel_precision =  0.9943181818181818  rel_recall =  0.9615384615384616  rel_f1_value =  0.9776536312849162\n",
            "ent_precision =  0.2556818181818182  ent_recall =  0.24725274725274726  ent_f1_value =  0.25139664804469275\n",
            "--*--*--Multiply label Triplets--*--*--\n",
            "gold_num =  326  pred_num =  141  right_num =  111  relation_right_num =  120  entity_right_num =  141\n",
            "precision =  0.7872340425531915  recall =  0.34049079754601225  f1_value =  0.4753747323340471\n",
            "rel_precision =  0.851063829787234  rel_recall =  0.36809815950920244  rel_f1_value =  0.5139186295503212\n",
            "ent_precision =  1.0  ent_recall =  0.4325153374233129  ent_f1_value =  0.6038543897216274\n",
            "--*--*--Overlapping Triplets--*--*--\n",
            "gold_num =  364  pred_num =  178  right_num =  129  relation_right_num =  142  entity_right_num =  171\n",
            "precision =  0.7247191011235955  recall =  0.3543956043956044  f1_value =  0.47601476014760147\n",
            "rel_precision =  0.797752808988764  rel_recall =  0.3901098901098901  rel_f1_value =  0.5239852398523985\n",
            "ent_precision =  0.9606741573033708  ent_recall =  0.4697802197802198  ent_f1_value =  0.6309963099630996\n",
            "gold_num =  546  pred_num =  354  right_num =  174  relation_right_num =  317  entity_right_num =  216\n",
            "precision =  0.4915254237288136  recall =  0.31868131868131866  f1_value =  0.38666666666666666\n",
            "rel_precision =  0.8954802259887006  rel_recall =  0.5805860805860806  rel_f1_value =  0.7044444444444444\n",
            "ent_precision =  0.6101694915254238  ent_recall =  0.3956043956043956  ent_f1_value =  0.48\n",
            "Achieving Best Result on Validation Set.\n",
            "Best result on validation set is 0.386667 achieving at epoch 4.\n",
            "--*--*--Num of Gold Triplet is 1--*--*--\n",
            "gold_num =  449  pred_num =  445  right_num =  145  relation_right_num =  444  entity_right_num =  146\n",
            "precision =  0.3258426966292135  recall =  0.32293986636971045  f1_value =  0.3243847874720358\n",
            "rel_precision =  0.9977528089887641  rel_recall =  0.9888641425389755  rel_f1_value =  0.9932885906040269\n",
            "ent_precision =  0.32808988764044944  ent_recall =  0.32516703786191536  ent_f1_value =  0.32662192393736017\n",
            "--*--*--Normal Triplets--*--*--\n",
            "gold_num =  439  pred_num =  436  right_num =  145  relation_right_num =  436  entity_right_num =  145\n",
            "precision =  0.33256880733944955  recall =  0.33029612756264237  f1_value =  0.33142857142857146\n",
            "rel_precision =  1.0  rel_recall =  0.9931662870159453  rel_f1_value =  0.9965714285714287\n",
            "ent_precision =  0.33256880733944955  ent_recall =  0.33029612756264237  ent_f1_value =  0.33142857142857146\n",
            "--*--*--Multiply label Triplets--*--*--\n",
            "gold_num =  0  pred_num =  0  right_num =  0  relation_right_num =  0  entity_right_num =  0\n",
            "precision =  -1  recall =  -1  f1_value =  -1\n",
            "rel_precision =  -1  rel_recall =  -1  rel_f1_value =  -1\n",
            "ent_precision =  -1  ent_recall =  -1  ent_f1_value =  -1\n",
            "--*--*--Overlapping Triplets--*--*--\n",
            "gold_num =  10  pred_num =  9  right_num =  0  relation_right_num =  8  entity_right_num =  1\n",
            "precision =  0.0  recall =  0.0  f1_value =  -1\n",
            "rel_precision =  0.8888888888888888  rel_recall =  0.8  rel_f1_value =  0.8421052631578948\n",
            "ent_precision =  0.1111111111111111  ent_recall =  0.1  ent_f1_value =  0.10526315789473685\n",
            "gold_num =  449  pred_num =  445  right_num =  145  relation_right_num =  444  entity_right_num =  146\n",
            "precision =  0.3258426966292135  recall =  0.32293986636971045  f1_value =  0.3243847874720358\n",
            "rel_precision =  0.9977528089887641  rel_recall =  0.9888641425389755  rel_f1_value =  0.9932885906040269\n",
            "ent_precision =  0.32808988764044944  ent_recall =  0.32516703786191536  ent_f1_value =  0.32662192393736017\n",
            "{'precision': 0.3258426966292135, 'recall': 0.32293986636971045, 'f1': 0.3243847874720358}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "2K_Ry7LzP9rW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}